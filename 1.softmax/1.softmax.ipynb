{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Preparation\n",
    "import libs\n",
    "\n",
    "load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dl_utils.data_utils import load_CIFAR10\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # set default size of plots\n",
    "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\"\n",
    "\n",
    "\n",
    "def get_CIFAR10_data(\n",
    "    num_training=49000, num_validation=1000, num_test=1000, num_dev=500\n",
    "):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = \"/home/slipstream/Projects/dl/dl_utils/datasets/cifar-10-batches-py\"\n",
    "\n",
    "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "    try:\n",
    "        del X_train, y_train\n",
    "        del X_test, y_test\n",
    "        print(\"Clear previously loaded data.\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "\n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "\n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print(\"Train data shape: \", X_train.shape)\n",
    "print(\"Train labels shape: \", y_train.shape)\n",
    "print(\"Validation data shape: \", X_val.shape)\n",
    "print(\"Validation labels shape: \", y_val.shape)\n",
    "print(\"Test data shape: \", X_test.shape)\n",
    "print(\"Test labels shape: \", y_test.shape)\n",
    "print(\"dev data shape: \", X_dev.shape)\n",
    "print(\"dev labels shape: \", y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Loss Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.344710\n",
      "loss: 2.325581\n",
      "loss: 2.411054\n",
      "loss: 2.313342\n",
      "loss: 2.352015\n",
      "loss: 2.372320\n",
      "loss: 2.380199\n",
      "loss: 2.328782\n",
      "loss: 2.357075\n",
      "loss: 2.385624\n",
      "the correct loss should be close to: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "from builtins import range\n",
    "import time\n",
    "\n",
    "\n",
    "def softmax_loss_naive(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Softmax loss function, naive implementation (with loops)\n",
    "\n",
    "    Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "    of N examples.\n",
    "\n",
    "    Inputs:\n",
    "    - W: A numpy array of shape (D, C) containing weights.\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "      that X[i] has label c, where 0 <= c < C.\n",
    "    - reg: (float) regularization strength, which contributes to the loss by a\n",
    "      regularization term: 0.5 * reg * np.sum(W * W)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss as single float\n",
    "    - gradient with respect to weights W; an array of same shape as W\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "\n",
    "    ##########################################################################\n",
    "    # TODO: Compute the softmax loss and its gradient using explicit loops.  #\n",
    "    # Store the loss in loss and the gradient in dW.                         #\n",
    "    ##########################################################################\n",
    "    # *****START OF YOUR CODE *****\n",
    "\n",
    "    C = W.shape[1]\n",
    "    N, D = X.shape\n",
    "    for i in range(N):\n",
    "        scores = X[i].dot(W)\n",
    "        scores -= np.max(scores)\n",
    "        probs = np.exp(scores) / np.sum(np.exp(scores))\n",
    "        loss += -np.log(probs[y[i]])\n",
    "\n",
    "        for j in range(C):\n",
    "            dW[:, j] += (probs[j] - (j == y[i])) * X[i]\n",
    "\n",
    "    loss /= N\n",
    "    dW /= N\n",
    "    loss += 0.5 * reg * np.sum(W * W)\n",
    "    dW += reg * W\n",
    "\n",
    "    # *****END OF YOUR CODE *****\n",
    "\n",
    "    return loss, dW\n",
    "\n",
    "\n",
    "for _ in range(10):\n",
    "    # Generate a random softmax weight matrix and use it to compute the loss.\n",
    "    W = np.random.randn(3073, 10) * 0.0001\n",
    "    loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "    print(\"loss: %f\" % loss)\n",
    "\n",
    "print(\"the correct loss should be close to: %f\" % (-np.log(0.1)))  # (why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 4.938181 analytic: 4.938181, relative error: 2.126411e-08\n",
      "numerical: -0.953480 analytic: -0.953480, relative error: 3.017346e-08\n",
      "numerical: -1.029442 analytic: -1.029442, relative error: 2.733967e-08\n",
      "numerical: 3.085417 analytic: 3.085417, relative error: 1.567481e-08\n",
      "numerical: 0.118929 analytic: 0.118929, relative error: 3.830553e-07\n",
      "numerical: -2.967244 analytic: -2.967244, relative error: 1.455346e-08\n",
      "numerical: 0.573974 analytic: 0.573974, relative error: 5.591943e-08\n",
      "numerical: 0.973372 analytic: 0.973372, relative error: 5.293806e-09\n",
      "numerical: 2.394666 analytic: 2.394666, relative error: 4.340752e-08\n",
      "numerical: 0.227554 analytic: 0.227554, relative error: 3.323408e-07\n",
      "numerical: 2.748838 analytic: 2.748838, relative error: 3.483465e-08\n",
      "numerical: 0.118313 analytic: 0.118313, relative error: 4.194923e-07\n",
      "numerical: -3.410575 analytic: -3.410575, relative error: 9.752059e-09\n",
      "numerical: 1.049818 analytic: 1.049817, relative error: 6.199807e-08\n",
      "numerical: -2.576451 analytic: -2.576451, relative error: 3.662729e-09\n",
      "numerical: -3.100470 analytic: -3.100470, relative error: 4.021431e-09\n",
      "numerical: 1.190615 analytic: 1.190615, relative error: 8.294667e-09\n",
      "numerical: 3.139999 analytic: 3.139999, relative error: 1.698208e-08\n",
      "numerical: 2.251457 analytic: 2.251456, relative error: 2.729102e-08\n",
      "numerical: -2.955905 analytic: -2.955905, relative error: 1.478696e-08\n"
     ]
    }
   ],
   "source": [
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# Use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from dl_utils.gradient_check import grad_check_sparse\n",
    "\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.385624e+00 computed in 0.026582s\n",
      "vectorized loss: 2.385624e+00 computed in 0.004051s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now implement a vectorized version in softmax_loss_vectorized.\n",
    "def softmax_loss_vectorized(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Softmax loss function, vectorized version.\n",
    "    Inputs and outputs are the same as softmax_loss_naive.\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
    "    # Store the loss in loss and the gradient in dW.                            #\n",
    "    #############################################################################\n",
    "    # *****START OF YOUR CODE *****\n",
    "\n",
    "    C = W.shape[1]\n",
    "    N, D = X.shape\n",
    "\n",
    "    scores = X @ W\n",
    "    scores -= np.max(scores, axis=1, keepdims=True)\n",
    "    probs = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n",
    "    loss = np.sum(-np.log(probs[np.arange(N), y]))\n",
    "\n",
    "    dW = X.T @ (probs - np.eye(C)[y])\n",
    "\n",
    "    loss /= N\n",
    "    dW /= N\n",
    "    loss += 0.5 * reg * np.sum(W * W)\n",
    "    dW += reg * W\n",
    "\n",
    "    # *****END OF YOUR CODE *****\n",
    "\n",
    "    return loss, dW\n",
    "\n",
    "\n",
    "# The two versions (naive and vectorized) should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print(\"naive loss: %e computed in %fs\" % (loss_naive, toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print(\"vectorized loss: %e computed in %fs\" % (loss_vectorized, toc - tic))\n",
    "\n",
    "# use the Frobenius norm to compare the two versions of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord=\"fro\")\n",
    "print(\"Loss difference: %f\" % np.abs(loss_naive - loss_vectorized))\n",
    "print(\"Gradient difference: %f\" % grad_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        learning_rate=1e-3,\n",
    "        reg=1e-5,\n",
    "        num_iters=100,\n",
    "        batch_size=200,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train this classifier using stochastic gradient descent.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "          training samples each of dimension D.\n",
    "        - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
    "          means that X[i] has label 0 <= c < C for C classes.\n",
    "        - learning_rate: (float) learning rate for optimization.\n",
    "        - reg: (float) regularization strength.\n",
    "        - num_iters: (integer) max number of steps to take when optimizing\n",
    "        - batch_size: (integer) number of training examples to use at each step.\n",
    "        - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "        Outputs:\n",
    "        A list containing the value of the loss function at each training iteration.\n",
    "        \"\"\"\n",
    "        num_train, dim = X.shape\n",
    "        num_classes = (\n",
    "            np.max(y) + 1\n",
    "        )  # assume y takes values 0...K-1 where K is number of classes\n",
    "        if self.W is None:\n",
    "            # lazily initialize W\n",
    "            self.W = 0.001 * np.random.randn(dim, num_classes)\n",
    "\n",
    "        # Run stochastic gradient descent to optimize W\n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "\n",
    "            #########################################################################\n",
    "            # TODO:                                                                 #\n",
    "            # Sample batch_size elements from the training data and their           #\n",
    "            # corresponding labels to use in this round of gradient descent.        #\n",
    "            # Store the data in X_batch and their corresponding labels in           #\n",
    "            # y_batch; after sampling X_batch should have shape (batch_size, dim)   #\n",
    "            # and y_batch should have shape (batch_size,)                           #\n",
    "            #                                                                       #\n",
    "            # Hint: Use np.random.choice to generate indices.                       #\n",
    "            #########################################################################\n",
    "            # *****START OF YOUR CODE *****\n",
    "\n",
    "            idx = np.random.choice(num_train, batch_size)\n",
    "            X_batch = X[idx]\n",
    "            y_batch = y[idx]\n",
    "\n",
    "            # perform parameter update\n",
    "            #########################################################################\n",
    "            # TODO:                                                                 #\n",
    "            # Update the weights self.W using the gradient and the learning rate.   #\n",
    "            #########################################################################\n",
    "            # *****START OF YOUR CODE *****\n",
    "\n",
    "            loss, grad = self.loss(X_batch, y_batch, reg)\n",
    "            self.W -= learning_rate * grad\n",
    "\n",
    "            # *****END OF YOUR CODE *****\n",
    "\n",
    "            if verbose and it % 100 == 0:\n",
    "                print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this linear classifier to predict labels for\n",
    "        data points.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "          training samples each of dimension D.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
    "          array of length N, and each element is an integer giving the predicted\n",
    "          class.\n",
    "        \"\"\"\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        ###########################################################################\n",
    "        # TODO:                                                                   #\n",
    "        # Implement this method. Store the predicted labels in y_pred.            #\n",
    "        ###########################################################################\n",
    "        # *****START OF YOUR CODE *****\n",
    "\n",
    "        scores = X @ self.W\n",
    "        y_pred = np.argmax(scores, axis=1)\n",
    "\n",
    "        # *****END OF YOUR CODE *****\n",
    "        return y_pred\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        # return softmax_loss_vectorized(self.W, X_batch, y_batch, reg)\n",
    "        return softmax_loss_naive(self.W, X_batch, y_batch, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "test": "test"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 20.882288\n",
      "iteration 100 / 1000: loss 15.368358\n",
      "iteration 200 / 1000: loss 12.335218\n",
      "iteration 300 / 1000: loss 10.312426\n",
      "iteration 400 / 1000: loss 8.599302\n",
      "iteration 500 / 1000: loss 7.398833\n",
      "iteration 600 / 1000: loss 6.241856\n",
      "iteration 700 / 1000: loss 5.583079\n",
      "iteration 800 / 1000: loss 4.703070\n",
      "iteration 900 / 1000: loss 4.037365\n",
      "softmax on raw pixels final test set accuracy: 0.370000\n"
     ]
    }
   ],
   "source": [
    "# Train the Softmax Classifer on the CIFAR10 dataset\n",
    "\n",
    "# Provided as a reference. You may change these hyperparameters\n",
    "learning_rates = 1e-6\n",
    "regularization_strengths = 1e3\n",
    "\n",
    "softmax = Softmax()\n",
    "softmax.train(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    learning_rates,\n",
    "    regularization_strengths,\n",
    "    num_iters=1000,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred = softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print(\"softmax on raw pixels final test set accuracy: %f\" % (test_accuracy,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
